* A Crash Course on Why AI-Alignment Matters [For a Popular Audience]
** Watch These Two TED Talks
- [[https://www.youtube.com/watch?v=8nt3edWLgIg][Can we build AI without losing control over it?]] - Sam Harris 
- [[https://www.youtube.com/watch?v=MnT1xgZgkpk&t=1s][What happens when our computers get smarter than we are?]] - Nick Bostrom
** Read These Blogposts by Tim Urban and the Reply
- WaitButWhy on AI Safety: [[https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html][Part 1]] and [[https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html][Part 2]]
- [[http://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/][A Reply by Luke Muehlhauser correcting a few things]]
** Read More about Real Research on AI Safety
- [[https://80000hours.org/career-reviews/artificial-intelligence-risk-research/][80000hours.org Profile on AI Safety Research]]

* Books
- [[https://en.wikipedia.org/wiki/Superintelligence%3A_Paths%2C_Dangers%2C_Strategies][Superintelligence: Paths, Dangers, Strategies]] by Nick Bostrom
- [[https://en.wikipedia.org/wiki/Life_3.0][Life 3.0]] by Max Tegmark
- [[https://www.goodreads.com/book/show/39947993-artificial-intelligence-safety-and-security?ac=1&from_search=true][Artificial Intelligence Safety and Security]] by Roman Yampolskiy (Editor)

* Talks
- [[https://www.youtube.com/watch?v=8nt3edWLgIg][Can we build AI without losing control over it?]] - Sam Harris 
- [[https://www.youtube.com/watch?v=MnT1xgZgkpk&t=1s][What happens when our computers get smarter than we are?]] - Nick Bostrom
- [[https://www.youtube.com/watch?v=2LRwvU6gEbA][How to get empowered, not overpowered, by AI]] - Max Tegmark

* Blogposts
- [[https://thinkingwires.com/posts/2017-07-05-risks.html][Risks of Artificial Intelligence]] by Johannes Heidecke

* Papers
** Research Agendas
- [[https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616][Paul Christiano's Agenda]] summarised by Ajeya Cotra
- [[https://agentfoundations.org/item?id=1816][The Learning-Theoretic AI Alignment Research Agenda]] by Vadim Kosoy
- [[The Learning-Theoretic AI Alignment Research Agenda][MIRI Machine Learning Agenda]] by Jessica Taylor and Eliezer Yudkowsky and Patrick LaVictoire and Andrew Critch
- [[https://intelligence.org/files/TechnicalAgenda.pdf][MIRI Agent Foundations Agenda]] by Nate Soares and Benya Fallenstein
- [[https://arxiv.org/abs/1606.06565][Concrete Problems in AI Safety]] by Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man√©
** Literature Reviews
- [[https://arxiv.org/abs/1805.01109][AGI Safety Literature Review]] by Tom Everitt, Gary Lea, Marcus Hutter
- [[www.tomeveritt.se/papers/2018-thesis.pdf][Towards Safe Artificial General Intelligence]] - Tom Everitt's PhD Thesis
** Technical Papers
*** Agent Foundations
*** Machine Learning

* Newsletters
- [[https://rohinshah.com/alignment-newsletter/][Alignment Newsletter]] by Rohin Shah

* Institutes/ Research Groups
- [[http://futureoflife.org/][Future of Life Institute]]
- [[https://www.fhi.ox.ac.uk/][Future of Humanity Institute]]
- [[https://intelligence.org/][Machine Intelligence Research Institute]]
- [[https://ought.org/][Ought]]
- [[https://openai.com/][OpenAI]]
- [[https://medium.com/@deepmindsafetyresearch][DeepMind Safety Team]]
- [[https://humancompatible.ai/][Center for Human-Compatible AI]]

* Communities
- [[https://www.alignmentforum.org/][Alignment Forum]]

* Podcasts
- AI Alignment Podcast by Lucas Perry [Future of Life Institute]
- 80000hours Podcast by Rob Wiblin

* Frameworks/ Environments
* Other Lists Like This
- [[https://vkrakovna.wordpress.com/ai-safety-resources/#communities][AI Safety Resources by Victoria Krakovna]]
- [[https://humancompatible.ai/bibliography][CHAI Bibliography]]
- [[https://80000hours.org/ai-safety-syllabus/][80000hours.org Syllabus for AI Safety]]
